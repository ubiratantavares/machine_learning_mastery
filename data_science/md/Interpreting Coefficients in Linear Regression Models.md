
[Link](https://machinelearningmastery.com/interpreting-coefficients-in-linear-regression-models/)

Linear regression models are foundational in machine learning. Merely fitting a straight line and reading the coefficient tells a lot. But how do we extract and interpret the coefficients from these models to understand their impact on predicted outcomes? This post will demonstrate how one can interpret coefficients by exploring various scenarios. We’ll explore the analysis of a single numerical feature, examine the role of categorical variables, and unravel the complexities introduced when these features are combined. Through this exploration, we aim to equip you with the skills needed to leverage linear regression models effectively, enhancing your analytical capabilities across different data-driven domains.
## Overview

This post is divided into three parts; they are:

- Interpreting Coefficients in Linear Models with a Single Numerical Feature
- Interpreting Coefficients in Linear Models with a Single Categorical Feature
- Discussion on Combining Numerical and Categorical Features

## Interpreting Coefficients in Linear Models with a Single Numerical Feature

In this section, we focus on a single numerical feature from the Ames Housing dataset, “GrLivArea” (above-ground living area in square feet), to understand its direct impact on “SalePrice”. We employ K-Fold Cross-Validation to validate our model’s performance and extract the coefficient of “GrLivArea”. This coefficient estimates how much the house price is expected to increase for every additional square foot of living area under the assumption that all other factors remain constant. This is a fundamental aspect of linear regression analysis, ensuring that the effect of “GrLivArea” is isolated from other variables.

Here is how we set up our regression model to achieve this:

|                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1<br><br>2<br><br>3<br><br>4<br><br>5<br><br>6<br><br>7<br><br>8<br><br>9<br><br>10<br><br>11<br><br>12<br><br>13<br><br>14<br><br>15<br><br>16<br><br>17<br><br>18<br><br>19<br><br>20<br><br>21<br><br>22<br><br>23<br><br>24<br><br>25<br><br>26<br><br>27<br><br>28<br><br>29<br><br>30<br><br>31 | # Set up to obtain CV model performance and coefficient using K-Fold<br><br>import pandas aspd<br><br>import numpy asnp<br><br>from sklearn.linear_model import LinearRegression<br><br>from sklearn.model_selection import KFold<br><br>Ames=pd.read_csv("Ames.csv")<br><br>X=Ames[["GrLivArea"]].values  # get 2D matrix<br><br>y=Ames["SalePrice"].values    # get 1D vector<br><br>model=LinearRegression()<br><br>kf=KFold(n_splits=5)<br><br>coefs=[]<br><br>scores=[]<br><br># Manually perform K-Fold Cross-Validation<br><br>forfold,(train_index,test_index)inenumerate(kf.split(X),start=1):<br><br>    # Split the data into training and testing sets<br><br>    X_train,X_test=X[train_index],X[test_index]<br><br>    y_train,y_test=y[train_index],y[test_index]<br><br>    # Fit the model, obtain fold performance and coefficient<br><br>    model.fit(X_train,y_train)<br><br>    scores.append(model.score(X_test,y_test))<br><br>    coefs.append(model.coef_)<br><br>mean_score=np.mean(scores)<br><br>print(f"Mean CV R² = {mean_score:.4f}")<br><br>mean_coefs=np.mean(coefs)<br><br>print(f"Mean Coefficient = {mean_coefs:.4f}") |

The output from this code block provides two key pieces of information: the mean R² score across the folds and the mean coefficient for “GrLivArea.” The R² score gives us a general idea of how well our model fits the data across different subsets, indicating the model’s consistency and reliability. Meanwhile, the mean coefficient quantifies the average effect of “GrLivArea” on “SalePrice” across all the validation folds.

|   |   |
|---|---|
|1<br><br>2|Mean CV R² = 0.5127<br><br>Mean Coefficient = 110.5214|

The coefficient of “GrLivArea” can be directly interpreted as the price change per square foot. Specifically, it indicates that for each square foot increase in “GrLivArea,” the sale price of the house is expected to rise by approximately $110.52 (not to be confused with the price per square foot since the coefficient refers to the **marginal price**). Conversely, a decrease in living area by one square foot would typically lower the sale price by the same amount.

## Interpreting Coefficients in Linear Models with a Single Categorical Feature

While numerical features like “GrLivArea” can be directly used in our regression model, categorical features require a different approach. Proper encoding of these categorical variables is crucial for accurate model training and ensuring the results are interpretable. In this section, we’ll explore One Hot Encoding—a technique that prepares categorical variables for linear regression by transforming them into a format that is interpretable within the model’s framework. We will specifically focus on how to interpret the coefficients that result from these transformations, including the strategic selection of a reference category to simplify these interpretations.

Choosing an appropriate reference category when applying One Hot Encoding is crucial as it sets the baseline against which other categories are compared. This baseline category’s mean value often serves as the intercept in our regression model. Let’s explore the distribution of sale prices across neighborhoods to select a reference category that will make our model both interpretable and meaningful:

|   |   |
|---|---|
|1<br><br>2<br><br>3<br><br>4|# Rank neighborhoods by their mean sale price<br><br>Ames=pd.read_csv("Ames.csv")<br><br>neighbor_stats=Ames.groupby("Neighborhood")["SalePrice"].agg(["count","mean"]).sort_values(by="mean")<br><br>print(neighbor_stats.round(0).astype(int))|

This output will inform our choice by highlighting the neighborhoods with the lowest and highest average prices, as well as indicating the neighborhoods with sufficient data points (count) to ensure robust statistical analysis:

|   |   |
|---|---|
|1<br><br>2<br><br>3<br><br>4<br><br>5<br><br>6<br><br>7<br><br>8<br><br>9<br><br>10<br><br>11<br><br>12<br><br>13<br><br>14<br><br>15<br><br>16<br><br>17<br><br>18<br><br>19<br><br>20<br><br>21<br><br>22<br><br>23<br><br>24<br><br>25<br><br>26<br><br>27<br><br>28<br><br>29<br><br>30|count    mean<br><br>Neighborhood              <br><br>MeadowV          34   96836<br><br>BrDale           29  106095<br><br>IDOTRR           76  108103<br><br>BrkSide         103  126030<br><br>OldTown         213  126939<br><br>Edwards         165  133152<br><br>SWISU            42  133576<br><br>Landmrk           1  137000<br><br>Sawyer          139  137493<br><br>NPkVill          22  140743<br><br>Blueste          10  143590<br><br>NAmes           410  145087<br><br>Mitchel         104  162655<br><br>SawyerW         113  188102<br><br>Gilbert         143  189440<br><br>NWAmes          123  190372<br><br>Greens            8  193531<br><br>Blmngtn          23  196237<br><br>CollgCr         236  198133<br><br>Crawfor          92  202076<br><br>ClearCr          40  213981<br><br>Somerst         143  228762<br><br>Timber           54  242910<br><br>Veenker          23  251263<br><br>GrnHill           2  280000<br><br>StoneBr          43  305308<br><br>NridgHt         121  313662<br><br>NoRidge          67  326114|

Choosing a neighborhood like “MeadowV” as our reference sets a clear baseline, interpreting other neighborhoods’ coefficients straightforward: they show how much more expensive houses are than “MeadowV”.

Having identified “MeadowV” as our reference neighborhood, we are now ready to apply One Hot Encoding to the “Neighborhood” feature, explicitly excluding “MeadowV” to establish it as our baseline in the model. This step ensures that all subsequent neighborhood coefficients are interpreted in relation to “MeadowV,” providing a clear comparative analysis of house pricing across different areas. The next block of code will demonstrate this encoding process, fit a linear regression model using K-Fold cross-validation, and calculate the average coefficients and Y-intercept. These calculations will help quantify the additional value or deficit associated with each neighborhood compared to our baseline, offering actionable insights for market evaluation.

|   |   |
|---|---|
|1<br><br>2<br><br>3<br><br>4<br><br>5<br><br>6<br><br>7<br><br>8<br><br>9<br><br>10<br><br>11<br><br>12<br><br>13<br><br>14<br><br>15<br><br>16<br><br>17<br><br>18<br><br>19<br><br>20<br><br>21<br><br>22<br><br>23<br><br>24<br><br>25<br><br>26<br><br>27<br><br>28<br><br>29<br><br>30<br><br>31<br><br>32<br><br>33<br><br>34<br><br>35<br><br>36<br><br>37<br><br>38<br><br>39<br><br>40<br><br>41<br><br>42<br><br>43<br><br>44<br><br>45<br><br>46<br><br>47<br><br>48<br><br>49|# Build on initial set up and block of code above<br><br># Import OneHotEncoder to preprocess a categorical feature<br><br>from sklearn.preprocessing import OneHotEncoder<br><br># One Hot Encoding for "Neighborhood", Note: drop=["MeadowV"]<br><br>encoder=OneHotEncoder(sparse=False,drop=["MeadowV"])<br><br>X=encoder.fit_transform(Ames[["Neighborhood"]])<br><br>y=Ames["SalePrice"].values<br><br># Setup KFold and initialize storage<br><br>kf=KFold(n_splits=5)<br><br>scores=[]<br><br>coefficients=[]<br><br>intercept=[]<br><br># Perform the KFold cross-validation<br><br>fortrain_index,test_index inkf.split(X):<br><br>    X_train,X_test=X[train_index],X[test_index]<br><br>    y_train,y_test=y[train_index],y[test_index]<br><br>    model=LinearRegression()<br><br>    model.fit(X_train,y_train)<br><br>    # Append the results for each fold<br><br>    scores.append(model.score(X_test,y_test))<br><br>    coefficients.append(model.coef_)<br><br>    intercept.append(model.intercept_)<br><br>mean_score=np.mean(scores)<br><br>print(f"Mean CV R² = {mean_score:.4f}")<br><br>mean_coefficients=np.mean(coefficients,axis=0)<br><br>mean_intercept=np.mean(intercept)<br><br>print(f"Mean Y-intercept = {mean_intercept:.0f}")<br><br># Retrieve neighborhood names from the encoder, adjusting for the dropped category<br><br>neighborhoods=encoder.categories_[0]<br><br>if"MeadowV"inneighborhoods:<br><br>    neighborhoods=[name forname inneighborhoods ifname!="MeadowV"]<br><br># Create a DataFrame to nicely display neighborhoods with their average coefficients<br><br>import pandas aspd<br><br>coefficients_df=pd.DataFrame({<br><br>    "Neighborhood":neighborhoods,<br><br>    "Average Coefficient":mean_coefficients.round(0).astype(int)<br><br>})<br><br># Print or return the DataFrame<br><br>print(coefficients_df.sort_values(by="Average Coefficient").reset_index(drop=True))|

The mean R² will remain consistent at 0.5408 regardless of what feature we “dropped” when we One Hot Encoded.

The Y-intercept provides a specific quantitative benchmark. Representing the average sale price in “MeadowV,” this Y-intercept forms the foundational price level against which all other neighborhoods’ premiums or discounts are measured.

|   |   |
|---|---|
|1<br><br>2<br><br>3<br><br>4<br><br>5<br><br>6<br><br>7<br><br>8<br><br>9<br><br>10<br><br>11<br><br>12<br><br>13<br><br>14<br><br>15<br><br>16<br><br>17<br><br>18<br><br>19<br><br>20<br><br>21<br><br>22<br><br>23<br><br>24<br><br>25<br><br>26<br><br>27<br><br>28<br><br>29<br><br>30<br><br>31|Mean CV R² = 0.5408<br><br>Mean Y-intercept = 96827<br><br>   Neighborhood  Average Coefficient<br><br>0        BrDale                 9221<br><br>1        IDOTRR                11335<br><br>2       BrkSide                29235<br><br>3       OldTown                30092<br><br>4       Landmrk                31729<br><br>5       Edwards                36305<br><br>6         SWISU                36848<br><br>7        Sawyer                40645<br><br>8       NPkVill                43988<br><br>9       Blueste                46388<br><br>10        NAmes                48274<br><br>11      Mitchel                65851<br><br>12      SawyerW                91252<br><br>13      Gilbert                92627<br><br>14       NWAmes                93521<br><br>15       Greens                96641<br><br>16      Blmngtn                99318<br><br>17      CollgCr               101342<br><br>18      Crawfor               105258<br><br>19      ClearCr               116993<br><br>20      Somerst               131844<br><br>21       Timber               146216<br><br>22      Veenker               155042<br><br>23      GrnHill               183173<br><br>24      StoneBr               208096<br><br>25      NridgHt               216605<br><br>26      NoRidge               229423|

Each neighborhood’s coefficient, calculated relative to “MeadowV,” reveals its premium or deficit in house pricing. By setting “MeadowV” as the reference category in our One Hot Encoding process, its average sale price effectively becomes the intercept of our model. The coefficients calculated for other neighborhoods then measure the difference in expected sale prices relative to “MeadowV.” For instance, a positive coefficient for a neighborhood indicates that houses there are more expensive than those in “MeadowV” by the coefficient’s value, assuming all other factors are constant. This arrangement allows us to directly assess and compare the impact of different neighborhoods on the “SalePrice,” providing a clear and quantifiable understanding of each neighborhood’s relative market value.

## Discussion on Combining Numerical and Categorical Features

So far, we have examined how numerical and categorical features influence our predictions separately. However, real-world data often require more sophisticated models that can handle multiple types of data simultaneously to capture the complex relationships within the market. To achieve this, it is essential to become familiar with tools like the `ColumnTransformer`, which allows for the simultaneous processing of different data types, ensuring that each feature is optimally prepared for modeling. Let’s now demonstrate an example where we combine the living area (“GrLivArea”) with the neighborhood classification to see how these factors together affect our model performance.

|   |   |
|---|---|
|1<br><br>2<br><br>3<br><br>4<br><br>5<br><br>6<br><br>7<br><br>8<br><br>9<br><br>10<br><br>11<br><br>12<br><br>13<br><br>14<br><br>15<br><br>16<br><br>17<br><br>18<br><br>19<br><br>20<br><br>21<br><br>22<br><br>23<br><br>24<br><br>25<br><br>26<br><br>27<br><br>28<br><br>29<br><br>30<br><br>31<br><br>32<br><br>33<br><br>34<br><br>35<br><br>36<br><br>37<br><br>38<br><br>39<br><br>40<br><br>41<br><br>42<br><br>43<br><br>44<br><br>45<br><br>46<br><br>47<br><br>48<br><br>49<br><br>50<br><br>51<br><br>52<br><br>53<br><br>54<br><br>55<br><br>56<br><br>57<br><br>58<br><br>59<br><br>60<br><br>61<br><br>62<br><br>63<br><br>64<br><br>65<br><br>66<br><br>67<br><br>68<br><br>69<br><br>70<br><br>71|# Import the necessary libraries<br><br>import pandas aspd<br><br>import numpy asnp<br><br>from sklearn.model_selection import KFold<br><br>from sklearn.linear_model import LinearRegression<br><br>from sklearn.preprocessing import OneHotEncoder<br><br>from sklearn.compose import ColumnTransformer<br><br># Load data<br><br>Ames=pd.read_csv("Ames.csv")<br><br># Select features and target<br><br>features=Ames[["GrLivArea","Neighborhood"]]<br><br>target=Ames["SalePrice"]<br><br># Preprocess features using ColumnTransformer<br><br>preprocessor=ColumnTransformer(<br><br>    transformers=[<br><br>        ("num","passthrough",["GrLivArea"]),<br><br>        ("cat",OneHotEncoder(sparse=False,drop=["MeadowV"],handle_unknown="ignore"),["Neighborhood"])<br><br>    ])<br><br># Fit and transform the features<br><br>X_transformed=preprocessor.fit_transform(features)<br><br>feature_names=["GrLivArea"]+list(preprocessor.named_transformers_["cat"].get_feature_names_out())<br><br># Initialize KFold<br><br>kf=KFold(n_splits=5)<br><br># Initialize variables to store results<br><br>coefficients_list=[]<br><br>intercepts_list=[]<br><br>scores=[]<br><br># Perform the KFold cross-validation<br><br>fortrain_index,test_index inkf.split(X_transformed):<br><br>    X_train,X_test=X_transformed[train_index],X_transformed[test_index]<br><br>    y_train,y_test=target.iloc[train_index],target.iloc[test_index]<br><br>    # Initialize the linear regression model<br><br>    model=LinearRegression()<br><br>    # Fit the model on the training data<br><br>    model.fit(X_train,y_train)<br><br>    # Store coefficients and intercepts<br><br>    coefficients_list.append(model.coef_)<br><br>    intercepts_list.append(model.intercept_)<br><br>    # Evaluate the model<br><br>    scores.append(model.score(X_test,y_test))<br><br># Calculate the mean of scores, coefficients, and intercepts<br><br>average_score=np.mean(scores)<br><br>average_coefficients=np.mean(coefficients_list,axis=0)<br><br>average_intercept=np.mean(intercepts_list)<br><br># Display the average R² score and Y-Intercept across all folds<br><br># The Y-Intercept represents the baseline sale price in "MeadowV" with no additional living area<br><br>print(f"Mean CV R² Score of Combined Model: {average_score:.4f}")<br><br>print(f"Mean Y-intercept = {average_intercept:.0f}")<br><br># Create a DataFrame for the coefficients<br><br>df_coefficients=pd.DataFrame({<br><br>    "Feature":feature_names,<br><br>    "Average Coefficient":average_coefficients<br><br>    }).sort_values(by="Average Coefficient").reset_index(drop=True)<br><br># Display the DataFrame<br><br>print("Coefficients for Combined Model:")<br><br>print(df_coefficients)|

The code above should output:

|   |   |
|---|---|
|1<br><br>2<br><br>3<br><br>4<br><br>5<br><br>6<br><br>7<br><br>8<br><br>9<br><br>10<br><br>11<br><br>12<br><br>13<br><br>14<br><br>15<br><br>16<br><br>17<br><br>18<br><br>19<br><br>20<br><br>21<br><br>22<br><br>23<br><br>24<br><br>25<br><br>26<br><br>27<br><br>28<br><br>29<br><br>30<br><br>31<br><br>32<br><br>33|Mean CV R² Score of Combined Model: 0.7375<br><br>Mean Y-intercept = 11786<br><br>Coefficients for Combined Model:<br><br>                 Feature  Average Coefficient<br><br>0     Neighborhood_SWISU         -3728.929853<br><br>1    Neighborhood_IDOTRR         -1498.971239<br><br>2              GrLivArea            78.938757<br><br>3   Neighborhood_OldTown          2363.805796<br><br>4    Neighborhood_BrDale          6551.114637<br><br>5   Neighborhood_BrkSide         16521.117849<br><br>6   Neighborhood_Landmrk         16921.529665<br><br>7   Neighborhood_Edwards         17520.110407<br><br>8   Neighborhood_NPkVill         30034.541748<br><br>9     Neighborhood_NAmes         31717.960146<br><br>10   Neighborhood_Sawyer         32009.140024<br><br>11  Neighborhood_Blueste         39908.310031<br><br>12   Neighborhood_NWAmes         44409.237736<br><br>13  Neighborhood_Mitchel         48013.229999<br><br>14  Neighborhood_SawyerW         48204.606372<br><br>15  Neighborhood_Gilbert         49255.248193<br><br>16  Neighborhood_Crawfor         55701.500795<br><br>17  Neighborhood_ClearCr         61737.497483<br><br>18  Neighborhood_CollgCr         69781.161291<br><br>19  Neighborhood_Blmngtn         72456.245569<br><br>20  Neighborhood_Somerst         90020.562168<br><br>21   Neighborhood_Greens         90219.452164<br><br>22   Neighborhood_Timber         97021.781128<br><br>23  Neighborhood_Veenker         98829.786236<br><br>24  Neighborhood_NoRidge        120717.748175<br><br>25  Neighborhood_StoneBr        147811.849406<br><br>26  Neighborhood_NridgHt        150129.579392<br><br>27  Neighborhood_GrnHill        157858.199004|

Combining “GrLivArea” and “Neighborhood” into a single model has significantly improved the R² score, rising to 0.7375 from the individual scores of 0.5127 and 0.5408, respectively. This substantial increase illustrates that integrating multiple data types provides a more accurate reflection of the complex factors influencing real estate prices.

However, this integration introduces new complexities into the model. The interaction effects between features like “GrLivArea” and “Neighborhood” can significantly alter the coefficients. For instance, the coefficient for “GrLivArea” decreased from 110.52 in the single-feature model to 78.93 in the combined model. This change illustrates how the value of living area is influenced by the characteristics of different neighborhoods. Incorporating multiple variables requires adjustments in the coefficients to account for overlapping variances between predictors, resulting in coefficients that often differ from those in single-feature models.

The mean Y-intercept calculated for our combined model is $11,786. This value represents the predicted sale price for a house in the “MeadowV” neighborhood with the base living area (as accounted for by “GrLivArea”) adjusted to zero. This intercept serves as a foundational price point, enhancing our interpretation of how different neighborhoods compare to “MeadowV” in terms of cost, once adjusted for the size of the living area. Each neighborhood’s coefficient, therefore, informs us about the additional cost or savings relative to our baseline, “MeadowV,” providing clear and actionable insights into the relative value of properties across different areas.

## **Further** Reading

#### APIs

- [sklearn.compose.ColumnTransformer](https://12ft.io/proxy?q=https%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fgenerated%2Fsklearn.compose.ColumnTransformer.html) API

#### Tutorials

- [Interpreting Regression Coefficients](https://12ft.io/proxy?q=https%3A%2F%2Fwww.theanalysisfactor.com%2Finterpreting-regression-coefficients%2F) by Karen Grace-Martin

#### **Ames Housing Dataset & Data Dictionary**

- [Ames Dataset](https://12ft.io/proxy?q=https%3A%2F%2Fraw.githubusercontent.com%2FPadre-Media%2Fdataset%2Fmain%2FAmes.csv)
- [Ames Data Dictionary](https://12ft.io/proxy?q=https%3A%2F%2Fgithub.com%2FPadre-Media%2Fdataset%2Fblob%2Fmain%2FAmes%2520Data%2520Dictionary.txt)

## **Summary**

This post has guided you through interpreting coefficients in linear regression models with clear, practical examples using the Ames Housing dataset. We explored how different types of features—numerical and categorical—affect the predictability and clarity of models. Moreover, we addressed the challenges and benefits of combining these features, especially in the context of interpretation.

Specifically, you learned:

- **The Direct Impact of Single Numerical Features:** How the “GrLivArea” coefficient directly quantifies the increase in “SalePrice” for each additional square foot, providing a clear measure of its predictive value in a straightforward model.
- **Handling Categorical Variables:** The importance of One Hot Encoding in dealing with categorical features like “Neighborhood”, illustrating how choosing a baseline category impacts the interpretation of coefficients and sets a foundation for comparison across different areas.
- **Combining Features to Enhance Model Performance:** The integration of “GrLivArea” and “Neighborhood” not only improved the predictive accuracy (R² score) but also introduced a complexity that affects how each feature’s coefficient is interpreted. This part emphasized the trade-off between achieving high predictive accuracy and maintaining model interpretability, which is crucial for making informed decisions in the real estate market.

Do you have any questions? Please ask your questions in the comments below, and I will do my best to answer.